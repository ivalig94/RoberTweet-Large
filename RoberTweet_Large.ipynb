{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoberTweet-Large.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur3-eIphLkvO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers.utils.dummy_pt_objects import PreTrainedModel\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path= '/content/drive/MyDrive/TFG_IvanAlvarez/Experimentos/20220119-110518EnglishRobertalargeN0.8128163D5e-06'\n",
        "\n",
        "def create_dataloader_seq(inputs,masks,batch_size):\n",
        "  # Create the DataLoader for our data set\n",
        "  data =  TensorDataset(inputs, masks)\n",
        "  sampler=SequentialSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "def preprocessdata_tensortokenids_masks_usingencode_plus(data,tokenizer,MAX_LEN):\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "class ROBERTAClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=3, dropout_rate=0.1):\n",
        "        super(ROBERTAClassifier, self).__init__()\n",
        "                # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        H=50\n",
        "        D_out=n_classes\n",
        "        D_in=1024\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-large',return_dict=False)\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(D_in, D_in),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(D_in, D_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Feed input to BERT\n",
        "        outputs = self.roberta(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    predictions = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    print('all_logits:')\n",
        "    print(all_logits)\n",
        "    if NCLASES == 2:\n",
        "      # Apply softmax to calculate probabilities:\n",
        "      probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "      preds = np.where(probs[:, 1] > THRESHOLD, 1, 0)\n",
        "    else:\n",
        "      _,predicts=torch.max(all_logits, dim=1)\n",
        "      print('predicts:')\n",
        "      print(predicts)\n",
        "      predictions.extend(predicts)\n",
        "      preds = torch.stack(predictions).cpu()\n",
        "      #probs = F.softmax(all_logits, dim=1)\n",
        "      #preds = torch.argmax(all_logits, dim=1).flatten()\n",
        "    return preds\n",
        "\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "NCLASES=3\n",
        "device = torch.device(\"cpu\")\n",
        "tweets= pd.read_csv(\"/content/drive/MyDrive/TFG_IvanAlvarez/datasets/Nintendo/Nintendo100ConSentimentSinEtiquetar.csv\",encoding = 'utf8')\n",
        "X_test=tweets['review']\n",
        "\n",
        "\n",
        "model=ROBERTAClassifier()\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "MAX_LEN=128\n",
        "test_token_ids_tensors,test_masks_tensors=preprocessdata_tensortokenids_masks_usingencode_plus(X_test,tokenizer,MAX_LEN)\n",
        "test_dataloader_seq=create_dataloader_seq(test_token_ids_tensors,test_masks_tensors,1)\n",
        "load_checkpoint(file_path + '/model.pt', model)\n",
        "preds = bert_predict(model, test_dataloader_seq)\n",
        "\n",
        "  \n",
        "prediccionesTensor= preds.numpy()[0:]\n",
        "predicciones_letras=[]\n",
        "\n",
        "for value in prediccionesTensor:\n",
        "  if value==2:\n",
        "    predicciones_letras.append('positive')\n",
        "  elif value==1:\n",
        "    predicciones_letras.append('neutral')\n",
        "  elif value==0:\n",
        "    predicciones_letras.append('negative')   \n",
        "\n",
        "#print(prediccionesTensor)\n",
        "print(predicciones_letras)"
      ]
    }
  ]
}